# MLAlgorithmsCourse
This is the public repository for the ML Algorithms Course. In this course, we walk you through the ins and outs of each ML Algorithm. We did not build this course ourselves. We stood on the shoulders of giants. We think its only fair to credit all the resources we used to build this course, as we could not have created this course without the help of the ML community. 

## Flashcards
Please go to [Ankiweb.net](https://ankiweb.net) to download Anki and to sign up for account. Please go [here](https://github.com/PlayingNumbers/ML_Process_Course/blob/main/365datascience_ml_process_flashcards.apkg) to download the flashcards for this course.


## 1. Linear Regression
- [Linear Regression, Clearly Exlplained!!! by StatQuest](https://www.youtube.com/watch?v=nk2CQITm_eo&ab_channel=StatQuestwithJoshStarmer)
- [Linear Regression by Jim Frost](https://statisticsbyjim.com/regression/linear-regression/)
- [7 Classical Assumptions of Ordinary Least Squares (OLS) Linear Regression](https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/)
- [Gauss-Markov Theorem](https://statisticsbyjim.com/regression/gauss-markov-theorem-ols-blue/)
- [Linear Regression ‚Äî Detailed View](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)
- [Building Linear Regression (Least Squares) with Linear Algebra](https://towardsdatascience.com/building-linear-regression-least-squares-with-linear-algebra-2adf071dd5dd)
- [Linear Regression using Gradient Descent](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)

## 2. Regularization
- [what-are-l1-l2-and-elastic-net-regularization-in-neural-networks](https://github.com/christianversloot/machine-learning-articles/blob/main/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks.md)
- [When will L1 regularization work better than L2 and vice versa?](https://stats.stackexchange.com/questions/184019/when-will-l1-regularization-work-better-than-l2-and-vice-versa)
- [What is the difference between L1 and L2 regularization? How does it solve the problem of overfitting? Which regularizer to use and when?](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when)
- [What is elastic net regularization, and how does it solve the drawbacks of Ridge (ùêø2
) and Lasso (ùêø1
)?](https://stats.stackexchange.com/questions/184029/what-is-elastic-net-regularization-and-how-does-it-solve-the-drawbacks-of-ridge)
- [Ridge, LASSO, and ElasticNet Regression](https://towardsdatascience.com/ridge-lasso-and-elasticnet-regression-b1f9c00ea3a3)

## 3. Logistic Regression
- [The Intuitive Explanation of Logistic Regression](https://towardsdatascience.com/the-intuitive-explanation-of-logistic-regression-a0375b1bee54)
- [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8&ab_channel=StatQuestwithJoshStarmer)
- [Logistic Regression by Andrew Ng](https://www.youtube.com/watch?v=-la3q9d7AKQ&ab_channel=ArtificialIntelligence-AllinOne)
- [Logistic Regression by Amherst College](https://nhorton.people.amherst.edu/ips9/IPS_09_Ch14.pdf)
- [Intuition behind Log-loss score](https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a)
- [Log Loss Function by Alex Dyakonov](https://dasha.ai/en-us/blog/log-loss-function)

## 4. Gradient Descent
- [Gradient Descent From Scratch by Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/05/gradient-descent-from-scratch-complete-intuition/#:~:text=The%20intuition%20behind%20Gradient%20Descent&text=We%20have%20to%20find%20the,between%20actual%20and%20predicted%20values.)
- [Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w&ab_channel=3Blue1Brown)
- [Stochastic Gradient Descent, Clearly Explained!!! by Josh Starmer](https://www.youtube.com/watch?v=vMh0zPT0tLI&ab_channel=StatQuestwithJoshStarmer)
- [Gradient Descent Intuition ‚Äî How Machines Learn](https://medium.com/x8-the-ai-community/gradient-descent-intuition-how-machines-learn-d29ad7464453)
- [The Math and Intuition Behind Gradient Descent by Suraj Bansal](https://medium.datadriveninvestor.com/the-math-and-intuition-behind-gradient-descent-13c45f367a11)
- [Batch gradient descent versus stochastic gradient descent](https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent)

## 5. Decision Tree
- [Decision Trees Explained by James Thorn](https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6)
- [A Guide to Decision Trees for Beginners](https://www.kaggle.com/code/vipulgandhi/a-guide-to-decision-trees-for-beginners)
- [Decision and Classification Trees, Clearly Explained!!! by Josh Starmer](https://www.youtube.com/watch?v=_L39rN6gz7Y&ab_channel=StatQuestwithJoshStarmer)
- [Information Gain and Mutual Information for Machine Learning by Jason Brownlee](https://machinelearningmastery.com/information-gain-and-mutual-information/#:~:text=Mutual%20Information%20Related%3F-,What%20Is%20Information%20Gain%3F,samples%2C%20and%20hence%20less%20surprise.)
- [A Simple Explanation of Information Gain and Entropy by Victor Zhou](https://victorzhou.com/blog/information-gain/)
- [How to program a decision tree in Python from 0](https://anderfernandez.com/en/blog/code-decision-tree-python-from-scratch/)


## 6. Random Forest
- [Building Intuition for Random Forests by Rishi Sidhu](https://medium.com/x8-the-ai-community/building-intuition-for-random-forests-76d36fa28c5e)
- [An Introduction to Random Forest Algorithm for beginners](https://www.analyticsvidhya.com/blog/2021/10/an-introduction-to-random-forest-algorithm-for-beginners/)
- [Feature Importance in Random Forest](https://mljar.com/blog/feature-importance-in-random-forest/)
- [Detailed Explanation of Random Forests Features importance Bias](https://medium.com/@eng.mohammed.saad.18/detailed-explanation-of-random-forests-features-importance-bias-8755d26ac3bc)
- [Random Forest: A Complete Guide for Machine Learning by Niklas Donges](https://builtin.com/data-science/random-forest-algorithm)
- [Random Forest Simple Explanation by Will Koehrsen](https://williamkoehrsen.medium.com/random-forest-simple-explanation-377895a60d2d)
- [Why Choose Random Forest and Not Decision Trees](https://towardsai.net/p/machine-learning/why-choose-random-forest-and-not-decision-trees)
- [When to use Random Forest](https://datascience.stackexchange.com/questions/54751/when-to-use-random-forest)


## 7. Gradient Boosted Trees
- [The Intuition Behind Gradient Boosting & XGBoost by Bobby Tan](https://towardsdatascience.com/the-intuition-behind-gradient-boosting-xgboost-6d5eac844920)
- [Gradient Boosting Algorithm: A Complete Guide for Beginners](https://www.analyticsvidhya.com/blog/2021/09/gradient-boosting-algorithm-a-complete-guide-for-beginners/)
- [Gradient Boosting Trees vs. Random Forests](https://www.baeldung.com/cs/gradient-boosting-trees-vs-random-forests#:~:text=4.3.-,Advantages%20and%20Disadvantages,and%20start%20modeling%20the%20noise.)
- [Gradient Boosting In Classification: Not a Black Box Anymore!](https://blog.paperspace.com/gradient-boosting-for-classification/)
- [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)

# 8. XGBoost
- [XGBoost Paper](https://arxiv.org/abs/1603.02754)
- [A Gentle Introduction to XGBoost for Applied Machine Learning](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)
- [XGBoost A Scalable Tree Boosting System by Tianqi Chen](https://www.youtube.com/watch?v=Vly8xGnNiWs&ab_channel=RealDataScienceUSA%28formerlyDataScience.LA%29)
- [CatBoost vs. LightGBM vs. XGBoost](https://towardsdatascience.com/catboost-vs-lightgbm-vs-xgboost-c80f40662924)
- [XGBoost, LightGBM or CatBoost ‚Äî which boosting algorithm should I use?](https://medium.com/riskified-technology/xgboost-lightgbm-or-catboost-which-boosting-algorithm-should-i-use-e7fda7bb36bc)


