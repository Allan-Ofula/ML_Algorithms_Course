# MLAlgorithmsCourse
This is the public repository for the ML Algorithms Course. In this course, we walk you through the ins and outs of each ML Algorithm. We did not build this course ourselves. We stood on the shoulders of giants. We think its only fair to credit all the resources we used to build this course, as we could not have created this course without the help of the ML community. 

## Flashcards
Please go to [Ankiweb.net](https://ankiweb.net) to download Anki and to sign up for account. Please go [here](https://github.com/PlayingNumbers/ML_Process_Course/blob/main/365datascience_ml_process_flashcards.apkg) to download the flashcards for this course.


## 1. Linear Regression
- [Linear Regression, Clearly Exlplained!!! by StatQuest](https://www.youtube.com/watch?v=nk2CQITm_eo&ab_channel=StatQuestwithJoshStarmer)
- [Linear Regression by Jim Frost](https://statisticsbyjim.com/regression/linear-regression/)
- [7 Classical Assumptions of Ordinary Least Squares (OLS) Linear Regression](https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/)
- [Gauss-Markov Theorem](https://statisticsbyjim.com/regression/gauss-markov-theorem-ols-blue/)
- [Linear Regression ‚Äî Detailed View](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)
- [Building Linear Regression (Least Squares) with Linear Algebra](https://towardsdatascience.com/building-linear-regression-least-squares-with-linear-algebra-2adf071dd5dd)
- [Linear Regression using Gradient Descent](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)

## 2. Regularization
- [what-are-l1-l2-and-elastic-net-regularization-in-neural-networks](https://github.com/christianversloot/machine-learning-articles/blob/main/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks.md)
- [When will L1 regularization work better than L2 and vice versa?](https://stats.stackexchange.com/questions/184019/when-will-l1-regularization-work-better-than-l2-and-vice-versa)
- [What is the difference between L1 and L2 regularization? How does it solve the problem of overfitting? Which regularizer to use and when?](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when)
- [What is elastic net regularization, and how does it solve the drawbacks of Ridge (ùêø2
) and Lasso (ùêø1
)?](https://stats.stackexchange.com/questions/184029/what-is-elastic-net-regularization-and-how-does-it-solve-the-drawbacks-of-ridge)
- [Ridge, LASSO, and ElasticNet Regression](https://towardsdatascience.com/ridge-lasso-and-elasticnet-regression-b1f9c00ea3a3)

## 3. Logistic Regression
- [The Intuitive Explanation of Logistic Regression](https://towardsdatascience.com/the-intuitive-explanation-of-logistic-regression-a0375b1bee54)
- [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8&ab_channel=StatQuestwithJoshStarmer)
- [Logistic Regression by Andrew Ng](https://www.youtube.com/watch?v=-la3q9d7AKQ&ab_channel=ArtificialIntelligence-AllinOne)
- [Logistic Regression by Amherst College](https://nhorton.people.amherst.edu/ips9/IPS_09_Ch14.pdf)
- [Intuition behind Log-loss score](https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a)
- [Log Loss Function by Alex Dyakonov](https://dasha.ai/en-us/blog/log-loss-function)

## 4. Decision Tree
- [Decision Trees Explained by James Thorn](https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6)
- [A Guide to Decision Trees for Beginners](https://www.kaggle.com/code/vipulgandhi/a-guide-to-decision-trees-for-beginners)
- [Decision and Classification Trees, Clearly Explained!!! by Josh Starmer](https://www.youtube.com/watch?v=_L39rN6gz7Y&ab_channel=StatQuestwithJoshStarmer)
- [Information Gain and Mutual Information for Machine Learning by Jason Brownlee](https://machinelearningmastery.com/information-gain-and-mutual-information/#:~:text=Mutual%20Information%20Related%3F-,What%20Is%20Information%20Gain%3F,samples%2C%20and%20hence%20less%20surprise.)
- [A Simple Explanation of Information Gain and Entropy by Victor Zhou](https://victorzhou.com/blog/information-gain/)
- [How to program a decision tree in Python from 0](https://anderfernandez.com/en/blog/code-decision-tree-python-from-scratch/)
- 


