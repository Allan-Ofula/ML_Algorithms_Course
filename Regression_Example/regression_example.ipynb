{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithms - Regression Example\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "For this example, we are trying to predict how much an individual customer will spend during a black Friday sale. This type of modeling is particularly useful for pricing campaigns and creating deals for specific customers. \n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>City_Category</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00069042</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00248942</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00087842</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00085442</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000002</td>\n",
       "      <td>P00285442</td>\n",
       "      <td>M</td>\n",
       "      <td>55+</td>\n",
       "      <td>16</td>\n",
       "      <td>C</td>\n",
       "      <td>4+</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Product_ID Gender   Age  Occupation City_Category  \\\n",
       "0  1000001  P00069042      F  0-17          10             A   \n",
       "1  1000001  P00248942      F  0-17          10             A   \n",
       "2  1000001  P00087842      F  0-17          10             A   \n",
       "3  1000001  P00085442      F  0-17          10             A   \n",
       "4  1000002  P00285442      M   55+          16             C   \n",
       "\n",
       "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
       "0                          2               0                   3   \n",
       "1                          2               0                   1   \n",
       "2                          2               0                  12   \n",
       "3                          2               0                  12   \n",
       "4                         4+               0                   8   \n",
       "\n",
       "   Product_Category_2  Product_Category_3  Purchase  \n",
       "0                 NaN                 NaN      8370  \n",
       "1                 6.0                14.0     15200  \n",
       "2                 NaN                 NaN      1422  \n",
       "3                14.0                 NaN      1057  \n",
       "4                 NaN                 NaN      7969  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26-35    219587\n",
       "36-45    110013\n",
       "18-25     99660\n",
       "46-50     45701\n",
       "51-55     38501\n",
       "55+       21504\n",
       "0-17      15102\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Age.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we need to fix a few of the following columns:\n",
    "\n",
    "**Occupation**: convert to categorical\n",
    "\n",
    "**Maritial_status**: convert to categorical\n",
    "\n",
    "**product_categories**: convert to categorical\n",
    "\n",
    "**Stay_In_Current_City_Years**: convert to numeric\n",
    "\n",
    "**Age**: Format the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to categorical from numeric \n",
    "df.Occupation = pd.Categorical(df['Occupation'])\n",
    "df.Marital_Status = pd.Categorical(df['Marital_Status'])\n",
    "df.Product_Category_1 = pd.Categorical(df['Product_Category_1'])\n",
    "df.Product_Category_2 = pd.Categorical(df['Product_Category_2'])\n",
    "df.Product_Category_3 = pd.Categorical(df['Product_Category_3'])\n",
    "\n",
    "#change to numeric \n",
    "df.Stay_In_Current_City_Years = df.Stay_In_Current_City_Years.str.replace('+','').astype(int)\n",
    "\n",
    "\n",
    "# fix age \n",
    "def random_age_replace(df_in,old_col,new_col):\n",
    "    np.random.seed(1)\n",
    "    df_out = df_in.copy()\n",
    "    df_out[new_col] = df_out[old_col]\n",
    "    vals = df_out[old_col].value_counts() \n",
    "    for i in vals.index:\n",
    "        if len(i.split('-')) > 1:\n",
    "            low = int(i.split('-')[0])\n",
    "            high = int(i.split('-')[1])\n",
    "            size = df_out[df_out[new_col] == i].shape[0]\n",
    "            df_out.loc[df_out[old_col] == i, new_col] = np.random.randint(low,high,size)\n",
    "        else:\n",
    "            size = df_out[df_out[new_col] == i].shape[0]\n",
    "            df_out.loc[df_out[old_col] == i, new_col] = np.random.randint(55,90,size)\n",
    "    return df_out\n",
    "\n",
    "df = random_age_replace(df,'Age','New_Age')\n",
    "df.New_Age = df.New_Age.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Purchase.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(column=['New_Age'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User_ID                       0.000000\n",
       "Product_ID                    0.000000\n",
       "Gender                        0.000000\n",
       "Age                           0.000000\n",
       "Occupation                    0.000000\n",
       "City_Category                 0.000000\n",
       "Stay_In_Current_City_Years    0.000000\n",
       "Marital_Status                0.000000\n",
       "Product_Category_1            0.000000\n",
       "Product_Category_2            0.315666\n",
       "Product_Category_3            0.696727\n",
       "Purchase                      0.000000\n",
       "New_Age                       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data manipulation for each type of model \n",
    "## Linear Regression\n",
    "    # No missing values \n",
    "    # Remove outliers \n",
    "# Ridge\n",
    "# Lasso \n",
    "# Elastic Net \n",
    "## KNN \n",
    "    # feature scaling \n",
    "    # Imputation \n",
    "## SVM\n",
    "    # Remove Outliers \n",
    "    # Remove missing values \n",
    "    # Scaling \n",
    "## Decision Tree\n",
    "## Random Forest\n",
    "## Gradient Boosted Tree\n",
    "## XG Boost\n",
    "## ANN  \n",
    "    # Imputation \n",
    "    # scaling\n",
    "    \n",
    "# fix scaling / avoid leakage --> pipelines? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "(1) Data imputation\n",
    "\n",
    "(2) Imputing & Outlier Removal \n",
    "\n",
    "(3) Imputing and Scaling\n",
    "\n",
    "(4) Imputing, Scaling, and Outlier Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category',\n",
       "       'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1',\n",
       "       'Product_Category_2', 'Product_Category_3', 'Purchase', 'New_Age'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill imputed categories with value indicating it is nan - 21\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cols_to_impute = ['Product_Category_1',\n",
    "                  'Product_Category_2',\n",
    "                  'Product_Category_3']\n",
    "\n",
    "cols_to_category = ['Gender',\n",
    "                    'Occupation',\n",
    "                    'City_Category',\n",
    "                    'Marital_Status',\n",
    "                    'Product_Category_1',\n",
    "                    'Product_Category_2',\n",
    "                    'Product_Category_3']\n",
    "\n",
    "cols_to_scale = ['New_Age',\n",
    "                 'Stay_In_Current_City_Years']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute NaN's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_imputer = SimpleImputer(strategy='constant', fill_value=21)\n",
    "\n",
    "impute_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat_impute', categorical_imputer, cols_to_impute),\n",
    "    ],remainder = 'passthrough',\n",
    "    verbose_feature_names_out= False).set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Numeric to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def to_category(x):\n",
    "    return pd.DataFrame(x).astype(\"category\")\n",
    "\n",
    "to_category = FunctionTransformer(to_category)\n",
    "\n",
    "category_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat_trans', to_category, cols_to_category),\n",
    "    ],remainder = 'passthrough', verbose_feature_names_out= False).set_output(transform='pandas')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "#adjust for X & Y \n",
    "\n",
    "def z_score_removal(X,y, columns, z_score):\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    col_df = df[columns]    \n",
    "    z_scores = scipy.stats.zscore(col_df).abs()\n",
    "    outliers = (z_scores.max(axis=1) > z_score)\n",
    "    df_out = df[~outliers]\n",
    "    X_cleaned = df_out[X.columns]\n",
    "    y_cleaned = df_out.drop(X.columns, axis =1)\n",
    "    return X_cleaned, y_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "scaling_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('min_max', scaler, cols_to_scale),\n",
    "    ],remainder = 'passthrough', verbose_feature_names_out= False).set_output(transform='pandas')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dummy variables \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_encoder = OneHotEncoder(sparse_output=False, drop='first',handle_unknown='ignore')\n",
    "onehot_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', oh_encoder, cols_to_category),\n",
    "    ],remainder = 'passthrough', verbose_feature_names_out= False).set_output(transform='pandas')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only remove outliers from the training set\n",
    "X_Reg_fin, y_reg_fin = z_score_removal(X_Reg,y_train,['New_Age','Purchase'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/ test split \n",
    "X = df.drop(['User_ID','Product_ID','Age','Purchase'], axis=1)\n",
    "y = df.loc[:,'Purchase']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Data Impuation & Outlier Removal (Linear Regression) ###############\n",
    "#outlier removal function\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "reg_pipe = Pipeline([\n",
    "    ('impute_trans', impute_preprocessor),\n",
    "    ('categorical_trans', category_preprocessor),\n",
    "    ('onehot', onehot_preprocessor)\n",
    "])\n",
    "\n",
    "X_Reg = reg_pipe.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running linear regression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sm_X = sm.add_constant(X_Reg_fin)\n",
    "sm_model_lr = sm.OLS(y_reg_fin, sm_X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Purchase   R-squared:                       0.619\n",
      "Model:                            OLS   Adj. R-squared:                  0.619\n",
      "Method:                 Least Squares   F-statistic:                     8401.\n",
      "Date:                Tue, 21 Mar 2023   Prob (F-statistic):               0.00\n",
      "Time:                        07:17:00   Log-Likelihood:            -3.7158e+06\n",
      "No. Observations:              398046   AIC:                         7.432e+06\n",
      "Df Residuals:                  397968   BIC:                         7.433e+06\n",
      "Df Model:                          77                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "const                       1.249e+04    136.494     91.513      0.000    1.22e+04    1.28e+04\n",
      "Gender_M                     -49.5079     10.584     -4.677      0.000     -70.253     -28.763\n",
      "Occupation_1                 -24.6776     19.285     -1.280      0.201     -62.476      13.121\n",
      "Occupation_2                   4.9395     23.031      0.214      0.830     -40.201      50.080\n",
      "Occupation_3                 181.2093     27.124      6.681      0.000     128.047     234.372\n",
      "Occupation_4                 123.8414     17.445      7.099      0.000      89.649     158.034\n",
      "Occupation_5                  46.7189     30.929      1.510      0.131     -13.902     107.340\n",
      "Occupation_6                 194.1100     25.635      7.572      0.000     143.866     244.354\n",
      "Occupation_7                  88.7673     18.036      4.922      0.000      53.417     124.118\n",
      "Occupation_8                -190.9228     85.236     -2.240      0.025    -357.983     -23.863\n",
      "Occupation_9                 144.4658     42.438      3.404      0.001      61.288     227.644\n",
      "Occupation_10                115.3606     40.305      2.862      0.004      36.364     194.357\n",
      "Occupation_11                 81.5086     31.907      2.555      0.011      18.973     144.044\n",
      "Occupation_12                259.5161     21.776     11.917      0.000     216.835     302.197\n",
      "Occupation_13                 15.6060     54.753      0.285      0.776     -91.708     122.920\n",
      "Occupation_14                198.8634     22.970      8.658      0.000     153.843     243.884\n",
      "Occupation_15                327.7555     31.687     10.344      0.000     265.650     389.861\n",
      "Occupation_16                161.0255     23.846      6.753      0.000     114.289     207.762\n",
      "Occupation_17                197.6668     20.264      9.754      0.000     157.950     237.384\n",
      "Occupation_18                 58.6330     40.582      1.445      0.149     -20.906     138.172\n",
      "Occupation_19               -319.2747     37.709     -8.467      0.000    -393.183    -245.366\n",
      "Occupation_20               -128.8519     21.195     -6.079      0.000    -170.394     -87.310\n",
      "City_Category_B              110.3013     10.697     10.311      0.000      89.335     131.267\n",
      "City_Category_C              456.6851     11.725     38.950      0.000     433.705     479.665\n",
      "Marital_Status_1             -53.5485      9.280     -5.770      0.000     -71.737     -35.360\n",
      "Product_Category_1_2.0     -1164.0317     29.122    -39.970      0.000   -1221.111   -1106.953\n",
      "Product_Category_1_3.0     -1629.9087     40.370    -40.374      0.000   -1709.033   -1550.784\n",
      "Product_Category_1_4.0     -1.029e+04     36.444   -282.354      0.000   -1.04e+04   -1.02e+04\n",
      "Product_Category_1_5.0     -6602.6665     16.377   -403.170      0.000   -6634.765   -6570.568\n",
      "Product_Category_1_6.0       813.7854     31.863     25.540      0.000     751.334     876.236\n",
      "Product_Category_1_7.0      1417.2873     65.244     21.723      0.000    1289.410    1545.165\n",
      "Product_Category_1_8.0     -5354.1690     17.383   -308.012      0.000   -5388.239   -5320.099\n",
      "Product_Category_1_9.0       995.0394    175.971      5.655      0.000     650.141    1339.937\n",
      "Product_Category_1_10.0     3867.2668     63.062     61.325      0.000    3743.668    3990.866\n",
      "Product_Category_1_11.0    -8083.2609     24.271   -333.045      0.000   -8130.831   -8035.691\n",
      "Product_Category_1_12.0    -1.148e+04     53.561   -214.279      0.000   -1.16e+04   -1.14e+04\n",
      "Product_Category_1_13.0    -1.217e+04     44.810   -271.646      0.000   -1.23e+04   -1.21e+04\n",
      "Product_Category_1_14.0      270.4167     82.394      3.282      0.001     108.928     431.905\n",
      "Product_Category_1_15.0     -310.7506     48.322     -6.431      0.000    -405.461    -216.040\n",
      "Product_Category_1_16.0      556.8362     39.016     14.272      0.000     480.366     633.306\n",
      "Product_Category_1_17.0    -2726.6714    134.044    -20.342      0.000   -2989.394   -2463.949\n",
      "Product_Category_1_18.0    -9849.1626     58.956   -167.060      0.000   -9964.714   -9733.611\n",
      "Product_Category_1_19.0    -1.287e+04     81.286   -158.289      0.000    -1.3e+04   -1.27e+04\n",
      "Product_Category_1_20.0    -1.252e+04     64.750   -193.309      0.000   -1.26e+04   -1.24e+04\n",
      "Product_Category_2_3.0      1212.7063     93.810     12.927      0.000    1028.842    1396.570\n",
      "Product_Category_2_4.0     -1524.2827     41.453    -36.771      0.000   -1605.529   -1443.036\n",
      "Product_Category_2_5.0      -584.3663     31.343    -18.644      0.000    -645.797    -522.935\n",
      "Product_Category_2_6.0        -8.3319     32.080     -0.260      0.795     -71.208      54.545\n",
      "Product_Category_2_7.0       710.7680    127.896      5.557      0.000     460.096     961.440\n",
      "Product_Category_2_8.0       253.9352     26.038      9.752      0.000     202.901     304.969\n",
      "Product_Category_2_9.0      -189.9992     48.095     -3.950      0.000    -284.264     -95.734\n",
      "Product_Category_2_10.0      980.6852     80.687     12.154      0.000     822.541    1138.829\n",
      "Product_Category_2_11.0     -369.2785     33.760    -10.938      0.000    -435.447    -303.110\n",
      "Product_Category_2_12.0     -390.4583     49.235     -7.930      0.000    -486.958    -293.958\n",
      "Product_Category_2_13.0     -179.9344     40.297     -4.465      0.000    -258.916    -100.953\n",
      "Product_Category_2_14.0     -139.3092     27.698     -5.030      0.000    -193.596     -85.023\n",
      "Product_Category_2_15.0     -201.4400     27.578     -7.304      0.000    -255.492    -147.388\n",
      "Product_Category_2_16.0      137.5357     27.993      4.913      0.000      82.671     192.400\n",
      "Product_Category_2_17.0      434.9065     36.941     11.773      0.000     362.504     507.309\n",
      "Product_Category_2_18.0      458.9521     66.611      6.890      0.000     328.397     589.507\n",
      "Product_Category_2_21.0      -31.7445     25.588     -1.241      0.215     -81.896      18.407\n",
      "Product_Category_3_4.0     -3463.6646    166.378    -20.818      0.000   -3789.761   -3137.568\n",
      "Product_Category_3_5.0       300.4036    137.740      2.181      0.029      30.438     570.369\n",
      "Product_Category_3_6.0       -53.2901    143.702     -0.371      0.711    -334.942     228.362\n",
      "Product_Category_3_8.0      1198.4934    139.039      8.620      0.000     925.980    1471.006\n",
      "Product_Category_3_9.0        73.6399    138.785      0.531      0.596    -198.374     345.653\n",
      "Product_Category_3_10.0     -711.9160    167.929     -4.239      0.000   -1041.051    -382.781\n",
      "Product_Category_3_11.0     -564.7689    154.563     -3.654      0.000    -867.707    -261.830\n",
      "Product_Category_3_12.0       37.6339    140.378      0.268      0.789    -237.503     312.771\n",
      "Product_Category_3_13.0     -651.8587    144.025     -4.526      0.000    -934.143    -369.575\n",
      "Product_Category_3_14.0     -129.8617    137.847     -0.942      0.346    -400.038     140.315\n",
      "Product_Category_3_15.0     -440.0690    136.282     -3.229      0.001    -707.177    -172.960\n",
      "Product_Category_3_16.0     -137.4694    137.190     -1.002      0.316    -406.358     131.420\n",
      "Product_Category_3_17.0      582.9555    138.756      4.201      0.000     310.997     854.914\n",
      "Product_Category_3_18.0      -73.4508    144.199     -0.509      0.610    -356.077     209.175\n",
      "Product_Category_3_21.0     -257.1478    136.277     -1.887      0.059    -524.246       9.950\n",
      "Stay_In_Current_City_Years     8.0037      3.383      2.366      0.018       1.373      14.635\n",
      "New_Age                       10.7236      0.513     20.902      0.000       9.718      11.729\n",
      "==============================================================================\n",
      "Omnibus:                    32875.465   Durbin-Watson:                   2.000\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            49730.065\n",
      "Skew:                          -0.653   Prob(JB):                         0.00\n",
      "Kurtosis:                       4.138   Cond. No.                     4.39e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.39e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(sm_model_lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model_lr = LinearRegression()\n",
    "\n",
    "model_lr.fit(X_Reg_fin, y_reg_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7531760.44977069 -7517331.37148732 -7500736.55293804]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lr_scores = cross_val_score(model_lr, X_Reg_fin, y_reg_fin, cv=3, scoring='neg_mean_squared_error')\n",
    "print(lr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Data Impuation, Outlier Removal, & Scaling (Ridge, Lasso, Elastic NEt, SVM) ###############\n",
    "rr_pipe = Pipeline([\n",
    "    ('impute_trans', impute_preprocessor),\n",
    "    ('categorical_trans', category_preprocessor),\n",
    "    ('feat_scaling', scaling_preprocessor),\n",
    "    ('onehot', onehot_preprocessor)\n",
    "])\n",
    "\n",
    "X_rr = rr_pipe.fit_transform(X_train)\n",
    "X_rr_fin, y_rr_fin = z_score_removal(X_rr,y_train,['New_Age','Purchase'],2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression + Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7536244.25629931 -7525693.93789678 -7514241.01295135]\n",
      "[-7531639.39414882 -7517378.35287102 -7500767.35231739]\n",
      "[-16459004.42121844 -16656169.89533992 -16585809.70477886]\n"
     ]
    }
   ],
   "source": [
    "#Code for running ridge regression \n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet \n",
    "\n",
    "lasso_model = Lasso()\n",
    "ridge_model = Ridge()\n",
    "en_model = ElasticNet() \n",
    "\n",
    "la_scores = cross_val_score(lasso_model, X_rr_fin, y_rr_fin, cv=3, scoring='neg_mean_squared_error')\n",
    "print(la_scores)\n",
    "r_scores = cross_val_score(ridge_model, X_rr_fin, y_rr_fin, cv=3, scoring='neg_mean_squared_error')\n",
    "print(r_scores)\n",
    "en_scores = cross_val_score(en_model, X_rr_fin, y_rr_fin, cv=3, scoring='neg_mean_squared_error')\n",
    "print(en_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Lasso and Ridge Regression\n",
    "\n",
    "L1 vs L2 Regularization: L1 regularization (Lasso) tends to produce sparse solutions, driving some coefficients to zero and performing feature selection, while L2 regularization (Ridge) produces solutions with small coefficients but does not drive them to zero, distributing the weights among correlated features.\n",
    "\n",
    "Relevant Parameters: \n",
    "- **alpha**: Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization.\n",
    "- **fit_intercept**: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e., data is expected to be centered).\n",
    "- **normalize**: This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the L2-norm.\n",
    "- **max_iter**: Maximum number of iterations for conjugate gradient solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha value: 1\n",
      "Best negative mean squared error: -7516742.3746946845\n",
      "Best model test R^2 score: 0.619105268744679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.144e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.178e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.144e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.026e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.042e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.025e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.868e+11, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.608e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+11, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.148e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.550e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.692e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.462e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.636e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.595e+09, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.248e+09, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+09, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha value: 0.01\n",
      "Best negative mean squared error: -7516693.501085868\n",
      "Best model test R^2 score: 0.6191051558281523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alpha_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "param_grid = {'alpha': alpha_values}\n",
    "grid_search_ridge = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_ridge.fit(X_rr_fin, y_rr_fin)\n",
    "test_score_ridge = grid_search_ridge.best_estimator_.score(X_rr_fin, y_rr_fin)\n",
    "\n",
    "\n",
    "print(f\"Best alpha value: {grid_search_ridge.best_params_['alpha']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_ridge.best_score_}\")\n",
    "print(f\"Best model test R^2 score: {test_score_ridge}\")\n",
    "\n",
    "\n",
    "param_grid = {'alpha': alpha_values}\n",
    "grid_search_lasso = GridSearchCV(lasso_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_lasso.fit(X_rr_fin, y_rr_fin)\n",
    "test_score_lasso = grid_search_lasso.best_estimator_.score(X_rr_fin, y_rr_fin)\n",
    "\n",
    "print(f\"Best alpha value: {grid_search_lasso.best_params_['alpha']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_lasso.best_score_}\")\n",
    "print(f\"Best model test R^2 score: {test_score_lasso}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Elastic Net Regression\n",
    "\n",
    "Elastic Net Regression is a linear regression model that combines L1 and L2 regularization, balancing the sparsity-inducing property of Lasso and the smoothness of Ridge Regression. It is particularly useful when there are multiple correlated features in the dataset.\n",
    "\n",
    "Relevant Parameters:\n",
    "\n",
    "- **alpha**: Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. `alpha` is the combined weight of both L1 and L2 regularization terms.\n",
    "- **l1_ratio**: The mixing parameter between L1 and L2 regularization, with a value between 0 and 1. A value of 0 corresponds to Ridge Regression (pure L2 penalty), and a value of 1 corresponds to Lasso Regression (pure L1 penalty). Values between 0 and 1 give a mix of both L1 and L2 regularization.\n",
    "- **fit_intercept**: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e., data is expected to be centered).\n",
    "- **normalize**: This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the L2-norm.\n",
    "- **max_iter**: Maximum number of iterations for the coordinate descent solver.\n",
    "\n",
    "By tuning `alpha` and `l1_ratio`, you can control the balance between L1 and L2 regularization and find the optimal trade-off for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.196e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.199e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.196e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.199e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.186e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.195e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.188e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.189e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.161e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.178e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.192e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.180e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.182e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.129e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.171e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.189e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.172e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.176e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.104e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.164e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.187e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.166e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.171e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.083e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.159e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.185e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.160e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.167e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.067e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.155e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.183e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.156e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.163e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.054e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.181e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.152e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.160e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.044e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.148e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.180e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.148e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.146e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.179e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.146e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.155e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.030e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.144e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.178e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.144e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.026e+12, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.210e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.213e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.212e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.210e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.213e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.191e+10, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.174e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.853e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.028e+10, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.118e+10, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.050e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.620e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.879e+10, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e+10, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.399e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.365e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.892e+10, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.284e+10, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.102e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.175e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.345e+10, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.010e+10, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.895e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.024e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.330e+09, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.385e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.688e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.451e+11, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.886e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.695e+09, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.581e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.355e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+11, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.283e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.275e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.942e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.190e+11, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.053e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.788e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.223e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.273e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.103e+11, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.950e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.780e+11, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.042e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.025e+12, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.868e+11, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.608e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+11, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.324e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.327e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.327e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.325e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.327e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.148e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.550e+11, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.692e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.462e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.636e+10, tolerance: 6.279e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.790e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.792e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.790e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.789e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.791e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.595e+09, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.248e+09, tolerance: 6.285e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+09, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.453e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.453e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.450e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.448e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.451e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.957e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.956e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.953e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.951e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.953e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.120e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.119e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.116e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.114e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.116e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.141e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.140e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.137e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.137e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.143e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.142e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.137e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.143e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.143e+12, tolerance: 6.285e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.137e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+12, tolerance: 6.279e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha value: 0.01\n",
      "Best l1 ratio value: 1.0\n",
      "Best negative mean squared error: -7516693.501085868\n",
      "Best model test R^2 score: 0.6191051558281523\n"
     ]
    }
   ],
   "source": [
    "#Elastic net \n",
    "alpha_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000]\n",
    "l1_ratios = np.linspace(0, 1, 11) \n",
    "\n",
    "param_grid_en = {'alpha': alpha_values, 'l1_ratio': l1_ratios}\n",
    "grid_search_en = GridSearchCV(en_model, param_grid_en, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_en.fit(X_rr_fin, y_rr_fin)\n",
    "test_score_en = grid_search_en.best_estimator_.score(X_rr_fin, y_rr_fin)\n",
    "\n",
    "\n",
    "print(f\"Best alpha value: {grid_search_en.best_params_['alpha']}\")\n",
    "print(f\"Best l1 ratio value: {grid_search_en.best_params_['l1_ratio']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_en.best_score_}\")\n",
    "print(f\"Best model test R^2 score: {test_score_en}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best l1 ratio value: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best l1 ratio value: {grid_search_en.best_params_['l1_ratio']}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - SVM Regression\n",
    "\n",
    "Support Vector Machine (SVM) Regression is a versatile machine learning algorithm that can be used for both linear and non-linear regression tasks. It aims to find the best-fitting hyperplane that has the largest distance (margin) between the support vectors and the hyperplane.\n",
    "\n",
    "Relevant Parameters:\n",
    "\n",
    "- **kernel**: Specifies the kernel function to be used in the algorithm. Possible options are 'linear', 'poly', 'rbf', 'sigmoid', and 'precomputed'. The choice of the kernel function depends on the nature of the data and the problem to be solved.\n",
    "- **C**: Regularization parameter (also called the cost parameter); must be a positive float. It determines the trade-off between achieving a low training error and a low testing error. In other words, it controls the balance between overfitting and underfitting. A smaller value of C creates a wider margin, which may result in more training errors but better generalization to the test data. A larger value of C creates a narrower margin, which may result in fewer training errors but poorer generalization to the test data.\n",
    "- **degree**: The degree of the polynomial kernel function ('poly'). Ignored by all other kernels. It is the degree of the polynomial used for the 'poly' kernel and determines the flexibility of the model.\n",
    "- **gamma**: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. If gamma is 'scale' (default), then it is calculated as 1 / (n_features * X.var()) for the input data X. If gamma is 'auto', then it is calculated as 1/n_features. A smaller gamma value will produce a more flexible model, while a larger gamma value will produce a more rigid model.\n",
    "- **coef0**: Independent term in the kernel function. It is only significant in 'poly' and 'sigmoid'. It controls the influence of higher degree terms in the polynomial and sigmoid kernels.\n",
    "- **shrinking**: Whether to use the shrinking heuristic. The shrinking heuristic is a technique used to speed up training by removing some of the support vectors that are not necessary for the final solution. True by default.\n",
    "- **epsilon**: Epsilon parameter in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated with the training loss function on the prediction error. Larger values of epsilon result in more tolerance for errors.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific regression problem and achieve a better balance between model complexity and generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10478304.0185813  -10438306.63053724 -10543011.19265019\n",
      " -10571788.61762295 -10482778.03383217]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'param_grid = {\\n    \\'kernel\\': [\\'linear\\', \\'rbf\\'],\\n    \\'C\\': [0.1, 1, 10],\\n    \\'gamma\\': [\\'scale\\', \\'auto\\', 0.1, 1, 10]\\n}\\ngrid_search_svm = GridSearchCV(svr, param_grid, scoring=\\'neg_mean_squared_error\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_svm.fit(X_rr_fin, y_rr_fin)\\ntest_score_svm = grid_search_svm.best_estimator_.score(X_rr_fin, y_rr_fin)\\n\\n\\nprint(f\"Best kernel value: {grid_search_svm.best_params_[\\'kernel\\']}\")\\nprint(f\"Best C value: {grid_search_svm.best_params_[\\'C\\']}\")\\nprint(f\"Best gamma value: {grid_search_svm.best_params_[\\'gamma\\']}\")\\nprint(f\"Best negative mean squared error: {grid_search_svm.best_score_}\")\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM Code \n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_model = SVR(kernel='linear', C=1.0)\n",
    "svm_scores = cross_val_score(svm_model, X_rr_fin, y_rr_fin, cv=5,scoring='neg_mean_squared_error')\n",
    "print(svm_scores)\n",
    "\n",
    "\"\"\"param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10]\n",
    "}\n",
    "grid_search_svm = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_svm.fit(X_rr_fin, y_rr_fin)\n",
    "test_score_svm = grid_search_svm.best_estimator_.score(X_rr_fin, y_rr_fin)\n",
    "\n",
    "\n",
    "print(f\"Best kernel value: {grid_search_svm.best_params_['kernel']}\")\n",
    "print(f\"Best C value: {grid_search_svm.best_params_['C']}\")\n",
    "print(f\"Best gamma value: {grid_search_svm.best_params_['gamma']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_svm.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### No Data Manipulation (Decision Tree, Random Forest, XGBoost) ###############\n",
    "tree_pipe = Pipeline([\n",
    "    ('categorical_trans', category_preprocessor),\n",
    "    ('onehot', onehot_preprocessor)\n",
    "])\n",
    "\n",
    "X_tree = tree_pipe.fit_transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Decision Tree\n",
    "\n",
    "Decision Trees are a popular machine learning algorithm used for both regression and classification tasks. They are easy to interpret and can naturally handle a mixture of continuous and categorical variables.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **criterion**: The function to measure the quality of a split. Supported criteria for regression are 'mse' (mean squared error) and 'friedman_mse' (improvement in mean squared error). For classification, supported criteria are 'gini' and 'entropy'.\n",
    "- **splitter**: The strategy used to choose the split at each node. Supported strategies are 'best' to choose the best split and 'random' to choose the best random split.\n",
    "- **max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Controlling the depth can help prevent overfitting.\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_weight_fraction_leaf**: The minimum weighted fraction of the sum total of weights required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "- **max_features**: The number of features to consider when looking for the best split. If None, then max_features=n_features.\n",
    "- **max_leaf_nodes**: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None, then unlimited number of leaf nodes.\n",
    "- **min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "- **min_impurity_split**: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-14580694.31561187 -14559593.94940835 -14815953.61983438\n",
      " -14852031.54953939 -14611572.68510791]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid = {\\n    \\'criterion\\': [\\'gini\\', \\'entropy\\'],\\n    \\'splitter\\': [\\'best\\', \\'random\\'],\\n    \\'max_depth\\': [None, 10, 20, 30, 40, 50],\\n    \\'min_samples_split\\': [2, 5, 10, 15, 20],\\n    \\'min_samples_leaf\\': [1, 2, 5, 10, 15],\\n    \\'min_weight_fraction_leaf\\': [0.0, 0.1, 0.2, 0.3, 0.4],\\n    \\'max_features\\': [None, \\'auto\\', \\'sqrt\\', \\'log2\\'],\\n    \\'max_leaf_nodes\\': [None, 10, 20, 30, 40, 50],\\n    \\'min_impurity_decrease\\': [0.0, 0.1, 0.2, 0.3, 0.4],\\n    \\'class_weight\\': [None, \\'balanced\\']\\n}\\n\\ngrid_search_dt = GridSearchCV(dt_model, param_grid_dt, scoring=\\'neg_mean_squared_error\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_dt.fit(X_tree, y_train)\\ntest_score_dt = grid_search_dt.best_estimator_.score(X_tree, y_train)\\n\\nprint(f\"Best max_depth value: {grid_search_dt.best_params_[\\'max_depth\\']}\")\\nprint(f\"Best min_samples_split value: {grid_search_dt.best_params_[\\'min_samples_split\\']}\")\\nprint(f\"Best min_samples_leaf value: {grid_search_dt.best_params_[\\'min_samples_leaf\\']}\")\\nprint(f\"Best negative mean squared error: {grid_search_dt.best_score_}\")\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decision Tree model \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_scores = cross_val_score(dt_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(dt_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(dt_model, param_grid_dt, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_dt.fit(X_tree, y_train)\n",
    "test_score_dt = grid_search_dt.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best max_depth value: {grid_search_dt.best_params_['max_depth']}\")\n",
    "print(f\"Best min_samples_split value: {grid_search_dt.best_params_['min_samples_split']}\")\n",
    "print(f\"Best min_samples_leaf value: {grid_search_dt.best_params_['min_samples_leaf']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_dt.best_score_}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method that constructs a multitude of decision trees at training time and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is highly flexible and can handle a wide variety of tasks.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **n_estimators**: The number of trees in the forest. Increasing the number of trees can improve the model's performance, but may also increase the computation time.\n",
    "- **criterion**: The function to measure the quality of a split. Supported criteria for regression are 'mse' (mean squared error) and 'mae' (mean absolute error). For classification, supported criteria are 'gini' and 'entropy'.\n",
    "- **max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Controlling the depth can help prevent overfitting.\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_weight_fraction_leaf**: The minimum weighted fraction of the sum total of weights required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "- **max_features**: The number of features to consider when looking for the best split. If None, then max_features=n_features. It can also be a float, int, or string ('auto', 'sqrt', or 'log2').\n",
    "- **max_leaf_nodes**: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None, then unlimited number of leaf nodes.\n",
    "- **min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "- **bootstrap**: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
    "- **oob_score**: Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9776669.96720507 -9717691.55464041 -9841291.06072307 -9867483.63379653\n",
      " -9673487.37217819]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid = {\\n    \\'n_estimators\\': [10, 50, 100, 200],\\n    \\'criterion\\': [\\'mse\\', \\'mae\\'],\\n    \\'max_depth\\': [None, 10, 20, 30, 40, 50],\\n    \\'min_samples_split\\': [2, 5, 10, 15, 20],\\n    \\'min_samples_leaf\\': [1, 2, 5, 10, 15],\\n    \\'min_weight_fraction_leaf\\': [0.0, 0.1, 0.2, 0.3, 0.4],\\n    \\'max_features\\': [None, \\'auto\\', \\'sqrt\\', \\'log2\\'],\\n    \\'max_leaf_nodes\\': [None, 10, 20, 30, 40, 50],\\n    \\'min_impurity_decrease\\': [0.0, 0.1, 0.2, 0.3, 0.4],\\n    \\'bootstrap\\': [True, False],\\n    \\'n_jobs\\': [-1]\\n}\\n\\ngrid_search_rf = GridSearchCV(rf_model, param_grid_rf, scoring=\\'neg_mean_squared_error\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_rf.fit(X_tree, y_train)\\ntest_score_rf = grid_search_rf.best_estimator_.score(X_tree, y_train)\\n\\nprint(f\"Best n_estimators value: {grid_search_rf.best_params_[\\'n_estimators\\']}\")\\nprint(f\"Best max_depth value: {grid_search_rf.best_params_[\\'max_depth\\']}\")\\nprint(f\"Best min_samples_split value: {grid_search_rf.best_params_[\\'min_samples_split\\']}\")\\nprint(f\"Best min_samples_leaf value: {grid_search_rf.best_params_[\\'min_samples_leaf\\']}\")\\nprint(f\"Best max_features value: {grid_search_rf.best_params_[\\'max_features\\']}\")\\nprint(f\"Best bootstrap value: {grid_search_rf.best_params_[\\'bootstrap\\']}\")\\nprint(f\"Best negative mean squared error: {grid_search_rf.best_score_}\")\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_scores = cross_val_score(rf_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(rf_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'criterion': ['mse', 'mae'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'bootstrap': [True, False],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_tree, y_train)\n",
    "test_score_rf = grid_search_rf.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best n_estimators value: {grid_search_rf.best_params_['n_estimators']}\")\n",
    "print(f\"Best max_depth value: {grid_search_rf.best_params_['max_depth']}\")\n",
    "print(f\"Best min_samples_split value: {grid_search_rf.best_params_['min_samples_split']}\")\n",
    "print(f\"Best min_samples_leaf value: {grid_search_rf.best_params_['min_samples_leaf']}\")\n",
    "print(f\"Best max_features value: {grid_search_rf.best_params_['max_features']}\")\n",
    "print(f\"Best bootstrap value: {grid_search_rf.best_params_['bootstrap']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_rf.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Gradient Boosted Regression\n",
    "\n",
    "Gradient Boosted Regression is an ensemble machine learning technique that builds a strong predictive model by combining a set of weak models (usually decision trees) in a sequential manner. At each iteration, a new weak model is added to the ensemble, and the existing models are updated to minimize the prediction errors.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **n_estimators**: The number of boosting stages (trees) to be constructed. Increasing the number of trees can improve the model's performance, but it can also lead to overfitting if too many trees are added.\n",
    "- **learning_rate**: The rate at which the contribution of each tree is shrunk. A smaller learning rate requires more trees in the ensemble but often leads to a more accurate model.\n",
    "- **max_depth**: The maximum depth of the individual regression estimators (trees). Deeper trees can capture more complex patterns in the data, but they can also lead to overfitting. Shallower trees are more likely to underfit the data.\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node in the decision tree. Higher values help prevent overfitting by avoiding splits on small subsets of the data.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node in the decision tree. Higher values help prevent overfitting by avoiding splits that result in small leaf nodes.\n",
    "- **max_features**: The number of features to consider when looking for the best split in the decision tree. Reducing the number of features can help prevent overfitting and reduce the computation time.\n",
    "- **subsample**: The fraction of samples to be used for fitting the individual base learners (trees). Smaller values introduce randomness and can help prevent overfitting. A value of 1.0 means all samples are used for each tree.\n",
    "- **loss**: The loss function to be optimized. Options include 'ls' (least squares), 'lad' (least absolute deviation), 'huber' (a combination of least squares and least absolute deviation), and 'quantile' (quantile regression). The choice of loss function depends on the specific problem and the desired robustness to outliers.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific regression problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9376363.26160314 -9296791.94147387 -9348482.6392767  -9367943.67449521\n",
      " -9336816.06006179]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid_gb = {\\n    \\'n_estimators\\': [10, 50, 100, 200],\\n    \\'learning_rate\\': [0.001, 0.01, 0.1, 0.2],\\n    \\'subsample\\': [0.5, 0.75, 1.0],\\n    \\'criterion\\': [\\'friedman_mse\\', \\'mse\\', \\'mae\\'],\\n    \\'min_samples_split\\': [2, 5, 10, 15, 20],\\n    \\'min_samples_leaf\\': [1, 2, 5, 10, 15],\\n    \\'min_weight_fraction_leaf\\': [0.0, 0.1, 0.2, 0.3, 0.4],\\n    \\'max_depth\\': [3, 10, 20, 30, 40, 50],\\n    \\'min_impurity_decrease\\': [0.0, 0.1, 0.2, 0.3, 0.4],\\n    \\'max_features\\': [None, \\'auto\\', \\'sqrt\\', \\'log2\\'],\\n    \\'max_leaf_nodes\\': [None, 10, 20, 30, 40, 50],\\n    \\'alpha\\': [0.1, 0.5, 0.9],\\n    \\'validation_fraction\\': [0.1, 0.2, 0.3],\\n    \\'n_iter_no_change\\': [None, 5, 10, 15],\\n    \\'tol\\': [1e-4, 1e-3, 1e-2, 1e-1]\\n}\\n\\ngrid_search_gb = GridSearchCV(gb_model, param_grid_gb, scoring=\\'neg_mean_squared_error\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_gb.fit(X_tree, y_train)\\ntest_score_gb = grid_search_gb.best_estimator_.score(X_tree, y_train)\\n\\nprint(f\"Best n_estimators value: {grid_search_gb.best_params_[\\'n_estimators\\']}\")\\nprint(f\"Best learning_rate value: {grid_search_gb.best_params_[\\'learning_rate\\']}\")\\nprint(f\"Best max_depth value: {grid_search_gb.best_params_[\\'max_depth\\']}\")\\nprint(f\"Best min_samples_split value: {grid_search_gb.best_params_[\\'min_samples_split\\']}\")\\nprint(f\"Best min_samples_leaf value: {grid_search_gb.best_params_[\\'min_samples_leaf\\']}\")\\nprint(f\"Best negative mean squared error: {grid_search_gb.best_score_}\")\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_scores = cross_val_score(gb_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(gb_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "    'subsample': [0.5, 0.75, 1.0],\n",
    "    'criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'max_depth': [3, 10, 20, 30, 40, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n",
    "    'alpha': [0.1, 0.5, 0.9],\n",
    "    'validation_fraction': [0.1, 0.2, 0.3],\n",
    "    'n_iter_no_change': [None, 5, 10, 15],\n",
    "    'tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(gb_model, param_grid_gb, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_gb.fit(X_tree, y_train)\n",
    "test_score_gb = grid_search_gb.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best n_estimators value: {grid_search_gb.best_params_['n_estimators']}\")\n",
    "print(f\"Best learning_rate value: {grid_search_gb.best_params_['learning_rate']}\")\n",
    "print(f\"Best max_depth value: {grid_search_gb.best_params_['max_depth']}\")\n",
    "print(f\"Best min_samples_split value: {grid_search_gb.best_params_['min_samples_split']}\")\n",
    "print(f\"Best min_samples_leaf value: {grid_search_gb.best_params_['min_samples_leaf']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_gb.best_score_}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - XGBoost\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework, offering several regularization techniques to prevent overfitting.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **learning_rate**: Boosting learning rate. Controls the contribution of each tree in the ensemble. Lower learning rates lead to more robust models but require more trees (n_estimators).\n",
    "- **n_estimators**: Number of boosting rounds to be run. Larger values result in more complex models but can increase the risk of overfitting.\n",
    "- **max_depth**: Maximum tree depth for base learners. Controls the depth of each individual tree in the ensemble. Deeper trees can capture more complex patterns, but may also overfit the data.\n",
    "- **min_child_weight**: Minimum sum of instance weight (hessian) needed in a child. Defines the minimum number of instances required for a node to be split.\n",
    "- **gamma**: Minimum loss reduction required to make a further partition on a leaf node of the tree. Controls the complexity of the tree by reducing the number of splits made.\n",
    "- **subsample**: Subsample ratio of the training instances. Setting it to a value less than 1.0 can help prevent overfitting.\n",
    "- **colsample_bytree**: Subsample ratio of columns when constructing each tree. A smaller value can reduce overfitting and speed up the training process.\n",
    "- **colsample_bylevel**: Subsample ratio of columns for each level. Specifies the fraction of features to choose for each level in the tree building process.\n",
    "- **colsample_bynode**: Subsample ratio of columns for each split. Specifies the fraction of features to choose for each split in the tree building process.\n",
    "- **reg_alpha**: L1 regularization term on weights. Controls the sparsity of feature weights, effectively performing feature selection.\n",
    "- **reg_lambda**: L2 regularization term on weights. Smoothens the weights, preventing extreme values and reducing the risk of overfitting.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8474467.84632277 -8337289.83554723 -8438951.98838967 -8474741.54863489\n",
      " -8335249.7428331 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid_xgb = {\\n    \\'n_estimators\\': [10, 50, 100, 200],\\n    \\'learning_rate\\': [0.001, 0.01, 0.1, 0.2],\\n    \\'max_depth\\': [3, 6, 9, 12],\\n    \\'min_child_weight\\': [1, 3, 5, 7],\\n    \\'gamma\\': [0.0, 0.1, 0.2, 0.3, 0.4],\\n    \\'subsample\\': [0.5, 0.75, 1.0],\\n    \\'colsample_bytree\\': [0.5, 0.75, 1.0],\\n    \\'reg_alpha\\': [0.0, 0.1, 0.5, 1.0],\\n    \\'reg_lambda\\': [0.0, 0.1, 0.5, 1.0],\\n    \\'scale_pos_weight\\': [1, 2, 3, 4],\\n    \\'booster\\': [\\'gbtree\\', \\'gblinear\\', \\'dart\\'],\\n    \\'tree_method\\': [\\'auto\\', \\'exact\\', \\'approx\\', \\'hist\\', \\'gpu_hist\\'],\\n    \\'grow_policy\\': [\\'depthwise\\', \\'lossguide\\'],\\n    \\'max_leaves\\': [0, 10, 20, 30, 40, 50],\\n    \\'sampling_method\\': [\\'uniform\\', \\'gradient_based\\']\\n}\\n\\nrandom_search_xgb = RandomizedSearchCV(xgb_model, param_grid_xgb, scoring=\\'neg_mean_squared_error\\', cv=5, n_jobs=-1, verbose=1, n_iter=50)\\nrandom_search_xgb.fit(X_tree, y_train)\\ntest_score_xgb = random_search_xgb.best_estimator_.score(X_tree, y_train)\\n\\nprint(f\"Best n_estimators value: {random_search_xgb.best_params_[\\'n_estimators\\']}\")\\nprint(f\"Best learning_rate value: {random_search_xgb.best_params_[\\'learning_rate\\']}\")\\nprint(f\"Best max_depth value: {random_search_xgb.best_params_[\\'max_depth\\']}\")\\nprint(f\"Best min_child_weight value: {random_search_xgb.best_params_[\\'min_child_weight\\']}\")\\nprint(f\"Best subsample value: {random_search_xgb.best_params_[\\'subsample\\']}\")\\nprint(f\"Best colsample_bytree value: {random_search_xgb.best_params_[\\'colsample_bytree\\']}\")\\nprint(f\"Best gamma value: {random_search_xgb.best_params_[\\'gamma\\']}\")\\nprint(f\"Best reg_alpha value: {random_search_xgb.best_params_[\\'reg_alpha\\']}\")\\nprint(f\"Best reg_lambda value: {random_search_xgb.best_params_[\\'reg_lambda\\']}\")\\nprint(f\"Best negative mean squared error: {random_search_xgb.best_score_}\")\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "xgb_model = XGBRegressor(tree_method='gpu_hist')\n",
    "xgb_scores = cross_val_score(xgb_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(xgb_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'subsample': [0.5, 0.75, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0],\n",
    "    'scale_pos_weight': [1, 2, 3, 4],\n",
    "    'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "    'tree_method': ['auto', 'exact', 'approx', 'hist', 'gpu_hist'],\n",
    "    'grow_policy': ['depthwise', 'lossguide'],\n",
    "    'max_leaves': [0, 10, 20, 30, 40, 50],\n",
    "    'sampling_method': ['uniform', 'gradient_based']\n",
    "}\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(xgb_model, param_grid_xgb, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1, n_iter=50)\n",
    "random_search_xgb.fit(X_tree, y_train)\n",
    "test_score_xgb = random_search_xgb.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best n_estimators value: {random_search_xgb.best_params_['n_estimators']}\")\n",
    "print(f\"Best learning_rate value: {random_search_xgb.best_params_['learning_rate']}\")\n",
    "print(f\"Best max_depth value: {random_search_xgb.best_params_['max_depth']}\")\n",
    "print(f\"Best min_child_weight value: {random_search_xgb.best_params_['min_child_weight']}\")\n",
    "print(f\"Best subsample value: {random_search_xgb.best_params_['subsample']}\")\n",
    "print(f\"Best colsample_bytree value: {random_search_xgb.best_params_['colsample_bytree']}\")\n",
    "print(f\"Best gamma value: {random_search_xgb.best_params_['gamma']}\")\n",
    "print(f\"Best reg_alpha value: {random_search_xgb.best_params_['reg_alpha']}\")\n",
    "print(f\"Best reg_lambda value: {random_search_xgb.best_params_['reg_lambda']}\")\n",
    "print(f\"Best negative mean squared error: {random_search_xgb.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Data Impuation & Scaling (KNN, Clustering, ANN) ###############\n",
    "knn_pipe = Pipeline([\n",
    "    ('impute_trans', impute_preprocessor),\n",
    "    ('categorical_trans', category_preprocessor),\n",
    "    ('feat_scaling', scaling_preprocessor),\n",
    "    ('onehot', onehot_preprocessor)\n",
    "])\n",
    "\n",
    "X_knn = knn_pipe.fit_transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - KNN\n",
    "\n",
    "k-Nearest Neighbors (KNN) is a simple, yet powerful, non-parametric supervised learning algorithm used for classification and regression. It assigns a new instance to the majority class or computes the mean (for regression tasks) of its k nearest neighbors in the feature space.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **n_neighbors**: Number of neighbors to use for the query. This is the main hyperparameter controlling the complexity of the KNN model. Larger values of k lead to smoother decision boundaries, while smaller values can capture more complex patterns but may overfit the data.\n",
    "- **weights**: Weight function used in prediction. There are two options: 'uniform' (all points in each neighborhood are weighted equally) and 'distance' (assign weights proportional to the inverse of the distance from the query point). Using 'distance' can help reduce the impact of noise in the data.\n",
    "- **algorithm**: Algorithm used to compute the nearest neighbors. Options include 'auto', 'ball_tree', 'kd_tree', and 'brute'. 'auto' will attempt to decide the most appropriate algorithm based on the values passed to fit() method. Choose the algorithm that best suits your data and computational requirements.\n",
    "- **leaf_size**: Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "- **p**: Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and for p = 2, it's equivalent to using euclidean_distance (l2). A larger value of p can help capture the specific geometry of your feature space.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10171022.76076127 -10137290.21508857 -10157129.61533172\n",
      " -10233634.94187408 -10025667.41653267]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid_knn = {\\n    \\'n_neighbors\\': [3, 5, 7, 9, 11],\\n    \\'weights\\': [\\'uniform\\', \\'distance\\'],\\n    \\'metric\\': [\\'euclidean\\', \\'manhattan\\', \\'minkowski\\']\\n}\\n\\ngrid_search_knn = GridSearchCV(knn_model, param_grid_knn, scoring=\\'neg_mean_squared_error\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_knn.fit(X_knn, y_train)\\ntest_score_knn = grid_search_knn.best_estimator_.score(X_knn, y_train)\\n\\nprint(f\"Best n_neighbors value: {grid_search_knn.best_params_[\\'n_neighbors\\']}\")\\nprint(f\"Best weights value: {grid_search_knn.best_params_[\\'weights\\']}\")\\nprint(f\"Best metric value: {grid_search_knn.best_params_[\\'metric\\']}\")\\nprint(f\"Best negative mean squared error: {grid_search_knn.best_score_}\")\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# KNN\n",
    "knn_model = KNeighborsRegressor()\n",
    "knn_scores = cross_val_score(knn_model, X_knn, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(knn_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(knn_model, param_grid_knn, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_knn.fit(X_knn, y_train)\n",
    "test_score_knn = grid_search_knn.best_estimator_.score(X_knn, y_train)\n",
    "\n",
    "print(f\"Best n_neighbors value: {grid_search_knn.best_params_['n_neighbors']}\")\n",
    "print(f\"Best weights value: {grid_search_knn.best_params_['weights']}\")\n",
    "print(f\"Best metric value: {grid_search_knn.best_params_['metric']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_knn.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - MLP Regression\n",
    "\n",
    "Multilayer Perceptron (MLP) Regression is a type of feedforward artificial neural network that can be used for regression tasks. MLPs consist of an input layer, one or more hidden layers, and an output layer. They learn to map input features to output values by adjusting the weights and biases of the network through a process called backpropagation.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **hidden_layer_sizes**: The number of hidden layers and the number of neurons in each hidden layer. This is specified as a tuple, e.g., (100,) means one hidden layer with 100 neurons, and (50, 50) means two hidden layers, each with 50 neurons. The choice of hidden layers and neurons can greatly impact the model's complexity and generalization performance.\n",
    "- **activation**: The activation function used for the hidden layers. Options include 'identity', 'logistic' (sigmoid), 'tanh', and 'relu' (rectified linear unit). The choice of activation function depends on the specific problem and the desired non-linearity in the model.\n",
    "- **solver**: The optimization algorithm used for weight and bias updates. Options include 'lbfgs', 'sgd' (stochastic gradient descent), and 'adam'. The choice of solver depends on the size and structure of the data and the desired computational efficiency.\n",
    "- **alpha**: L2 regularization parameter (also called the weight decay). A positive float value that adds a penalty term to the loss function, helping to prevent overfitting by reducing the magnitude of the weights.\n",
    "- **batch_size**: The size of the minibatches used for stochastic optimization. A smaller batch size introduces more randomness into the optimization process, which can help prevent overfitting and escape local minima.\n",
    "- **learning_rate**: The learning rate schedule for weight updates. Options include 'constant', 'invscaling', and 'adaptive'. The choice of learning rate schedule depends on the solver and the desired convergence behavior.\n",
    "- **learning_rate_init**: The initial learning rate used for weight updates. A smaller learning rate requires more iterations to converge but can result in a more accurate model.\n",
    "- **max_iter**: The maximum number of iterations for the solver. Increasing the number of iterations can improve the model's performance, but it can also lead to overfitting if too many iterations are used.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific regression problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8743081.13510717 -8663839.80750094 -8674514.68097197 -8768216.84234858\n",
      " -8668067.28328911]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'param_grid_ann = {\\n    \\'hidden_layer_sizes\\': [(50,), (100,), (50, 50), (100, 100)],\\n    \\'activation\\': [\\'identity\\', \\'logistic\\', \\'tanh\\', \\'relu\\'],\\n    \\'solver\\': [\\'lbfgs\\', \\'sgd\\', \\'adam\\'],\\n    \\'alpha\\': [0.0001, 0.001, 0.01, 0.1],\\n    \\'learning_rate\\': [\\'constant\\', \\'invscaling\\', \\'adaptive\\']\\n}\\n\\ngrid_search_ann = GridSearchCV(ann_model, param_grid_ann, scoring=\\'neg_mean_squared_error\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_ann.fit(X_tree, y_train)\\ntest_score_ann = grid_search_ann.best_estimator_.score(X_tree, y_train)\\n\\nprint(f\"Best hidden_layer_sizes value: {grid_search_ann.best_params_[\\'hidden_layer_sizes\\']}\")\\nprint(f\"Best activation value: {grid_search_ann.best_params_[\\'activation\\']}\")\\nprint(f\"Best solver value: {grid_search_ann.best_params_[\\'solver\\']}\")\\nprint(f\"Best alpha value: {grid_search_ann.best_params_[\\'alpha\\']}\")\\nprint(f\"Best learning_rate value: {grid_search_ann.best_params_[\\'learning_rate\\']}\")\\nprint(f\"Best negative mean squared error: {grid_search_ann.best_score_}\")\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# ANN\n",
    "ann_model = MLPRegressor(random_state=42)\n",
    "ann_scores = cross_val_score(ann_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(ann_scores)\n",
    "\n",
    "\"\"\"param_grid_ann = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "grid_search_ann = GridSearchCV(ann_model, param_grid_ann, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_ann.fit(X_tree, y_train)\n",
    "test_score_ann = grid_search_ann.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best hidden_layer_sizes value: {grid_search_ann.best_params_['hidden_layer_sizes']}\")\n",
    "print(f\"Best activation value: {grid_search_ann.best_params_['activation']}\")\n",
    "print(f\"Best solver value: {grid_search_ann.best_params_['solver']}\")\n",
    "print(f\"Best alpha value: {grid_search_ann.best_params_['alpha']}\")\n",
    "print(f\"Best learning_rate value: {grid_search_ann.best_params_['learning_rate']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_ann.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [5, 6] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test_reg = reg_pipe.transform(X_test)\n",
    "x_test_rr = rr_pipe.transform(X_test)\n",
    "x_test_Tree = tree_pipe.transform(X_test)\n",
    "x_test_knn = knn_pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ElasticNet()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ElasticNet</label><div class=\"sk-toggleable__content\"><pre>ElasticNet()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ElasticNet()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.fit(X_Reg_fin, y_reg_fin)\n",
    "lasso_model.fit(X_rr_fin, y_rr_fin) \n",
    "ridge_model.fit(X_rr_fin, y_rr_fin) \n",
    "en_model.fit(X_rr_fin, y_rr_fin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR(kernel='linear')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.fit(X_rr_fin, y_rr_fin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.fit(X_tree, y_train)\n",
    "rf_model.fit(X_tree, y_train)\n",
    "gb_model.fit(X_tree, y_train)\n",
    "xgb_model.fit(X_tree, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(random_state=42)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.fit(X_knn, y_train)\n",
    "ann_model.fit(X_knn, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but ElasticNet was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression neg MSE: -9486983.107230384\n",
      "Lasso neg MSE: -9473722.822497066\n",
      "Ridge neg MSE: -9423037.259717682\n",
      "Elastic Net neg MSE: -21705902.42434961\n",
      "SVM neg MSE: -12890401.585838215\n",
      "Decision Tree neg MSE: -20355368.203504704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest neg MSE: -10055282.719396153\n",
      "Gradient Boosting neg MSE: -9660802.37320467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost neg MSE: -9233708.258971043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN neg MSE: -10115305.914568326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjee/.local/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN neg MSE: -8738596.030275686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Linear Regression\n",
    "y_pred_lr = model_lr.predict(x_test_reg.values)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "print(f\"Linear Regression neg MSE: {-mse_lr}\")\n",
    "\n",
    "# Lasso\n",
    "y_pred_lasso = lasso_model.predict(x_test_rr.values)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(f\"Lasso neg MSE: {-mse_lasso}\")\n",
    "\n",
    "# Ridge\n",
    "y_pred_ridge = ridge_model.predict(x_test_rr.values)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(f\"Ridge neg MSE: {-mse_ridge}\")\n",
    "\n",
    "# Elastic Net\n",
    "y_pred_en = en_model.predict(x_test_rr.values)\n",
    "mse_en = mean_squared_error(y_test, y_pred_en)\n",
    "print(f\"Elastic Net neg MSE: {-mse_en}\")\n",
    "\n",
    "# SVM\n",
    "y_pred_svm = svm_model.predict(x_test_rr.values)\n",
    "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
    "print(f\"SVM neg MSE: {-mse_svm}\")\n",
    "\n",
    "# Decision Tree\n",
    "y_pred_dt = dt_model.predict(x_test_Tree.values)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree neg MSE: {-mse_dt}\")\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf = rf_model.predict(x_test_Tree.values)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Random Forest neg MSE: {-mse_rf}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "y_pred_gb = gb_model.predict(x_test_Tree.values)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting neg MSE: {-mse_gb}\")\n",
    "\n",
    "# XGBoost\n",
    "y_pred_xgb = xgb_model.predict(x_test_Tree.values)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost neg MSE: {-mse_xgb}\")\n",
    "\n",
    "# KNN\n",
    "y_pred_knn = knn_model.predict(x_test_knn.values)\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "print(f\"KNN neg MSE: {-mse_knn}\")\n",
    "\n",
    "# ANN\n",
    "y_pred_ann = ann_model.predict(x_test_knn.values)\n",
    "mse_ann = mean_squared_error(y_test, y_pred_ann)\n",
    "print(f\"ANN neg MSE: {-mse_ann}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
